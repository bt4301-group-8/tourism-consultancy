{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON data from file\n",
    "with open('reddit_posts.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_id</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>month_year</th>\n",
       "      <th>name</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>title</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>mentioned_countries</th>\n",
       "      <th>mentioned_cities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1jjuq8t</td>\n",
       "      <td>Stock_Rabbit_1901</td>\n",
       "      <td>2024-04-09 12:57:21.441380978+00:00</td>\n",
       "      <td>04-2024</td>\n",
       "      <td>t3_1jjuq8t</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>I’m going to China in May to visit a manufactu...</td>\n",
       "      <td>travel</td>\n",
       "      <td>China trip, advice for 6 nights</td>\n",
       "      <td>1.00</td>\n",
       "      <td>china</td>\n",
       "      <td>beijing, hangzhou, shanghai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1jh273f</td>\n",
       "      <td>Wit_in_the_Wild</td>\n",
       "      <td>2024-07-11 15:20:07.773072004+00:00</td>\n",
       "      <td>07-2024</td>\n",
       "      <td>t3_1jh273f</td>\n",
       "      <td>104</td>\n",
       "      <td>323</td>\n",
       "      <td>31F. Been a long-time lurker here, and with my...</td>\n",
       "      <td>solotravel</td>\n",
       "      <td>Fence-sitters, go take that leap! - Quit my jo...</td>\n",
       "      <td>0.90</td>\n",
       "      <td>singapore, malaysia, thailand, china</td>\n",
       "      <td>bangkok, hong kong, kuala lumpur, singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1j62q8v</td>\n",
       "      <td>OkMasterpiece260</td>\n",
       "      <td>2022-04-27 16:00:12.938550949+00:00</td>\n",
       "      <td>04-2022</td>\n",
       "      <td>t3_1j62q8v</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey backpackers! My name is Caitlin and I’m cu...</td>\n",
       "      <td>backpacking</td>\n",
       "      <td>Come to Thailand with me!!</td>\n",
       "      <td>0.09</td>\n",
       "      <td>singapore, thailand</td>\n",
       "      <td>singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1jcrmy9</td>\n",
       "      <td>Extension-Driver-136</td>\n",
       "      <td>2024-12-25 09:13:10.947906971+00:00</td>\n",
       "      <td>12-2024</td>\n",
       "      <td>t3_1jcrmy9</td>\n",
       "      <td>23</td>\n",
       "      <td>13</td>\n",
       "      <td>Hello everyone, \\n\\nI would like some help ple...</td>\n",
       "      <td>solotravel</td>\n",
       "      <td>Budgeting for Gap Year in SEA - Help please</td>\n",
       "      <td>0.84</td>\n",
       "      <td>vietnam, malaysia, thailand, indonesia, nepal,...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1eammc7</td>\n",
       "      <td>Loud-Gap8196</td>\n",
       "      <td>2022-11-28 14:02:49.658598900+00:00</td>\n",
       "      <td>11-2022</td>\n",
       "      <td>t3_1eammc7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>wanderlust</td>\n",
       "      <td>Luxury on a Budget! The Island Life on Koh Sam...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>thailand</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>1d3iy93</td>\n",
       "      <td>travelwithme24x7</td>\n",
       "      <td>2023-05-07 03:14:31.708024025+00:00</td>\n",
       "      <td>05-2023</td>\n",
       "      <td>t3_1d3iy93</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>wanderlust</td>\n",
       "      <td>Best Things To Do In Istanbul Turkey (Itinerar...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>turkey</td>\n",
       "      <td>istanbul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>1ihy9c2</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-09-02 19:40:51.985013008+00:00</td>\n",
       "      <td>09-2022</td>\n",
       "      <td>t3_1ihy9c2</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>Hey everyone, I am hoping to do a little bit o...</td>\n",
       "      <td>solotravel</td>\n",
       "      <td>Which option would you choose for your first m...</td>\n",
       "      <td>0.60</td>\n",
       "      <td>singapore, malaysia, thailand, indonesia, chin...</td>\n",
       "      <td>bali, hong kong, kyoto, osaka, seoul, singapor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>1ip7sjm</td>\n",
       "      <td>lifeisabop</td>\n",
       "      <td>2022-08-27 11:58:04.199906111+00:00</td>\n",
       "      <td>08-2022</td>\n",
       "      <td>t3_1ip7sjm</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Hi everyone! Am in a bit of an interesting sit...</td>\n",
       "      <td>solotravel</td>\n",
       "      <td>Accidentally granted two Vietnam eVisas?</td>\n",
       "      <td>1.00</td>\n",
       "      <td>vietnam</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311</th>\n",
       "      <td>1ig4d0y</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-11-15 01:36:19.114248991+00:00</td>\n",
       "      <td>11-2022</td>\n",
       "      <td>t3_1ig4d0y</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Hello,\\n\\nI am planning on making a trip to Vi...</td>\n",
       "      <td>solotravel</td>\n",
       "      <td>Vietnam Itinerary - few concerns</td>\n",
       "      <td>1.00</td>\n",
       "      <td>vietnam, philippines</td>\n",
       "      <td>hanoi, manila</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1312</th>\n",
       "      <td>1jiuy2r</td>\n",
       "      <td>Numinextherealone</td>\n",
       "      <td>2022-12-19 21:46:04.060967922+00:00</td>\n",
       "      <td>12-2022</td>\n",
       "      <td>t3_1jiuy2r</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey everyone,\\n\\nI'm currently refining my act...</td>\n",
       "      <td>backpacking</td>\n",
       "      <td>Comfort &gt; Weight: Help me build the ultimate a...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>japan</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1313 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     submission_id                author                         created_utc  \\\n",
       "0          1jjuq8t     Stock_Rabbit_1901 2024-04-09 12:57:21.441380978+00:00   \n",
       "1          1jh273f       Wit_in_the_Wild 2024-07-11 15:20:07.773072004+00:00   \n",
       "2          1j62q8v      OkMasterpiece260 2022-04-27 16:00:12.938550949+00:00   \n",
       "3          1jcrmy9  Extension-Driver-136 2024-12-25 09:13:10.947906971+00:00   \n",
       "4          1eammc7          Loud-Gap8196 2022-11-28 14:02:49.658598900+00:00   \n",
       "...            ...                   ...                                 ...   \n",
       "1308       1d3iy93      travelwithme24x7 2023-05-07 03:14:31.708024025+00:00   \n",
       "1309       1ihy9c2                  None 2022-09-02 19:40:51.985013008+00:00   \n",
       "1310       1ip7sjm            lifeisabop 2022-08-27 11:58:04.199906111+00:00   \n",
       "1311       1ig4d0y                  None 2022-11-15 01:36:19.114248991+00:00   \n",
       "1312       1jiuy2r     Numinextherealone 2022-12-19 21:46:04.060967922+00:00   \n",
       "\n",
       "     month_year        name  num_comments  score  \\\n",
       "0       04-2024  t3_1jjuq8t             5      6   \n",
       "1       07-2024  t3_1jh273f           104    323   \n",
       "2       04-2022  t3_1j62q8v             6      0   \n",
       "3       12-2024  t3_1jcrmy9            23     13   \n",
       "4       11-2022  t3_1eammc7             0      0   \n",
       "...         ...         ...           ...    ...   \n",
       "1308    05-2023  t3_1d3iy93             0      1   \n",
       "1309    09-2022  t3_1ihy9c2            46      3   \n",
       "1310    08-2022  t3_1ip7sjm             2      1   \n",
       "1311    11-2022  t3_1ig4d0y             2      1   \n",
       "1312    12-2022  t3_1jiuy2r             0      0   \n",
       "\n",
       "                                               selftext subreddit_name  \\\n",
       "0     I’m going to China in May to visit a manufactu...         travel   \n",
       "1     31F. Been a long-time lurker here, and with my...     solotravel   \n",
       "2     Hey backpackers! My name is Caitlin and I’m cu...    backpacking   \n",
       "3     Hello everyone, \\n\\nI would like some help ple...     solotravel   \n",
       "4                                                           wanderlust   \n",
       "...                                                 ...            ...   \n",
       "1308                                                        wanderlust   \n",
       "1309  Hey everyone, I am hoping to do a little bit o...     solotravel   \n",
       "1310  Hi everyone! Am in a bit of an interesting sit...     solotravel   \n",
       "1311  Hello,\\n\\nI am planning on making a trip to Vi...     solotravel   \n",
       "1312  Hey everyone,\\n\\nI'm currently refining my act...    backpacking   \n",
       "\n",
       "                                                  title  upvote_ratio  \\\n",
       "0                       China trip, advice for 6 nights          1.00   \n",
       "1     Fence-sitters, go take that leap! - Quit my jo...          0.90   \n",
       "2                            Come to Thailand with me!!          0.09   \n",
       "3           Budgeting for Gap Year in SEA - Help please          0.84   \n",
       "4     Luxury on a Budget! The Island Life on Koh Sam...          0.25   \n",
       "...                                                 ...           ...   \n",
       "1308  Best Things To Do In Istanbul Turkey (Itinerar...          1.00   \n",
       "1309  Which option would you choose for your first m...          0.60   \n",
       "1310           Accidentally granted two Vietnam eVisas?          1.00   \n",
       "1311                   Vietnam Itinerary - few concerns          1.00   \n",
       "1312  Comfort > Weight: Help me build the ultimate a...          0.50   \n",
       "\n",
       "                                    mentioned_countries  \\\n",
       "0                                                 china   \n",
       "1                  singapore, malaysia, thailand, china   \n",
       "2                                   singapore, thailand   \n",
       "3     vietnam, malaysia, thailand, indonesia, nepal,...   \n",
       "4                                              thailand   \n",
       "...                                                 ...   \n",
       "1308                                             turkey   \n",
       "1309  singapore, malaysia, thailand, indonesia, chin...   \n",
       "1310                                            vietnam   \n",
       "1311                               vietnam, philippines   \n",
       "1312                                              japan   \n",
       "\n",
       "                                       mentioned_cities  \n",
       "0                           beijing, hangzhou, shanghai  \n",
       "1           bangkok, hong kong, kuala lumpur, singapore  \n",
       "2                                             singapore  \n",
       "3                                                        \n",
       "4                                                        \n",
       "...                                                 ...  \n",
       "1308                                           istanbul  \n",
       "1309  bali, hong kong, kyoto, osaka, seoul, singapor...  \n",
       "1310                                                     \n",
       "1311                                      hanoi, manila  \n",
       "1312                                                     \n",
       "\n",
       "[1313 rows x 13 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert JSON to DataFrame\n",
    "df = pd.json_normalize(data)\n",
    "\n",
    "# Convert `created_utc` to datetime\n",
    "df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s', utc=True)\n",
    "\n",
    "# Convert list columns to comma-separated strings\n",
    "df['mentioned_countries'] = df['mentioned_countries'].apply(lambda x: ', '.join(x) if isinstance(x, list) else '')\n",
    "df['mentioned_cities'] = df['mentioned_cities'].apply(lambda x: ', '.join(x) if isinstance(x, list) else '')\n",
    "\n",
    "# Display DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates (avoid biased sentiment analysis)\n",
    "df = df.drop_duplicates(keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             created_utc   date_only        time_only\n",
      "0    2024-04-09 12:57:21.441380978+00:00  2024-04-09  12:57:21.441380\n",
      "1    2024-07-11 15:20:07.773072004+00:00  2024-07-11  15:20:07.773072\n",
      "2    2022-04-27 16:00:12.938550949+00:00  2022-04-27  16:00:12.938550\n",
      "3    2024-12-25 09:13:10.947906971+00:00  2024-12-25  09:13:10.947906\n",
      "4    2022-11-28 14:02:49.658598900+00:00  2022-11-28  14:02:49.658598\n",
      "...                                  ...         ...              ...\n",
      "1308 2023-05-07 03:14:31.708024025+00:00  2023-05-07  03:14:31.708024\n",
      "1309 2022-09-02 19:40:51.985013008+00:00  2022-09-02  19:40:51.985013\n",
      "1310 2022-08-27 11:58:04.199906111+00:00  2022-08-27  11:58:04.199906\n",
      "1311 2022-11-15 01:36:19.114248991+00:00  2022-11-15  01:36:19.114248\n",
      "1312 2022-12-19 21:46:04.060967922+00:00  2022-12-19  21:46:04.060967\n",
      "\n",
      "[1313 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert 'created_utc' to datetime format\n",
    "df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s', utc=True)\n",
    "\n",
    "# Split into separate 'date' and 'time' columns\n",
    "df['date_only'] = df['created_utc'].dt.date  # Extracts only the date (YYYY-MM-DD)\n",
    "df['time_only'] = df['created_utc'].dt.time  # Extracts only the time (HH:MM:SS)\n",
    "\n",
    "\n",
    "# Display the updated DataFrame with new columns\n",
    "print(df[['created_utc', 'date_only', 'time_only']])\n",
    "\n",
    "# # Extract day of the week (0 = Monday, 6 = Sunday)\n",
    "# df['day_of_week'] = df['created_utc'].dt.dayofweek\n",
    "\n",
    "# # Extract month\n",
    "# df['month'] = df['created_utc'].dt.month\n",
    "\n",
    "# # Extract year\n",
    "# df['year'] = df['created_utc'].dt.year\n",
    "\n",
    "# # Extract whether the post was made on a weekend (1 if weekend, 0 if weekday)\n",
    "# df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# # Extract weekday name (e.g., Monday, Tuesday)\n",
    "# df['weekday_name'] = df['created_utc'].dt.strftime('%A')\n",
    "\n",
    "# # Display the updated DataFrame with new columns\n",
    "# print(df[['created_utc', 'date_only', 'time_only', 'day_of_week', 'month', 'year', 'is_weekend', 'weekday_name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-English Rows:\n",
      "     selftext language\n",
      "4                 None\n",
      "8                 None\n",
      "19                None\n",
      "21                None\n",
      "29                None\n",
      "...       ...      ...\n",
      "1281              None\n",
      "1300              None\n",
      "1303              None\n",
      "1305              None\n",
      "1308              None\n",
      "\n",
      "[131 rows x 2 columns]\n",
      "\n",
      "Number of non-English rows: 131\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "# Function to detect language\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        if isinstance(text, str) and text.strip(): # Check if text is a non-empty string\n",
    "            return detect(text)\n",
    "        else:\n",
    "            return None # Handle cases with non-string or empty text\n",
    "    except LangDetectException:\n",
    "        return None  # Handle language detection errors\n",
    "\n",
    "# Apply language detection to 'selftext' column\n",
    "df['language'] = df['selftext'].apply(detect_language)\n",
    "\n",
    "# Filter out rows that are not English\n",
    "non_english_rows = df[df['language'] != 'en']\n",
    "\n",
    "# Display non-English rows\n",
    "print(\"Non-English Rows:\")\n",
    "print(non_english_rows[['selftext', 'language']])\n",
    "\n",
    "# Count the number of non-English rows\n",
    "non_english_count = len(non_english_rows)\n",
    "print(f\"\\nNumber of non-English rows: {non_english_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               selftext  \\\n",
      "0     I’m going to China in May to visit a manufactu...   \n",
      "1     31F. Been a long-time lurker here, and with my...   \n",
      "2     Hey backpackers! My name is Caitlin and I’m cu...   \n",
      "3     Hello everyone, \\n\\nI would like some help ple...   \n",
      "4                                                         \n",
      "...                                                 ...   \n",
      "1308                                                      \n",
      "1309  Hey everyone, I am hoping to do a little bit o...   \n",
      "1310  Hi everyone! Am in a bit of an interesting sit...   \n",
      "1311  Hello,\\n\\nI am planning on making a trip to Vi...   \n",
      "1312  Hey everyone,\\n\\nI'm currently refining my act...   \n",
      "\n",
      "                                        cleaned_caption  \n",
      "0     Im going to China in May to visit a manufactur...  \n",
      "1     31F. Been a longtime lurker here, and with my ...  \n",
      "2     Hey backpackers! My name is Caitlin and Im cur...  \n",
      "3     Hello everyone, I would like some help please!...  \n",
      "4                                                        \n",
      "...                                                 ...  \n",
      "1308                                                     \n",
      "1309  Hey everyone, I am hoping to do a little bit o...  \n",
      "1310  Hi everyone! Am in a bit of an interesting sit...  \n",
      "1311  Hello, I am planning on making a trip to Vietn...  \n",
      "1312  Hey everyone, Im currently refining my active ...  \n",
      "\n",
      "[1313 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # Remove emojis and other unwanted symbols\n",
    "    text = re.sub(r'[^\\w\\s,.!?;]', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Remove extra spaces and trim\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "df['cleaned_caption'] = df['selftext'].apply(clean_text)\n",
    "\n",
    "# Display cleaned captions\n",
    "print(df[['selftext', 'cleaned_caption']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "def analyze_and_translate_languages_with_google(dataset):\n",
    "    # Initialize the Google Translator\n",
    "    translator = Translator()\n",
    "\n",
    "    # Function to detect language using Google Translator\n",
    "    def detect_language(text):\n",
    "        try:\n",
    "            return translator.detect(text).lang\n",
    "        except Exception as e:\n",
    "            return 'unknown'\n",
    "\n",
    "    # Function to translate text to English\n",
    "    def translate_to_english(text):\n",
    "        try:\n",
    "            return translator.translate(text, dest='en').text\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "    # List to keep track of rows to drop\n",
    "    to_drop = []\n",
    "\n",
    "    # Iterate over the dataset and apply the logic for detecting and translating languages\n",
    "    for index, row in dataset.iterrows():\n",
    "        detected_language = detect_language(row[\"cleaned_caption\"])\n",
    "\n",
    "        if detected_language == 'en':\n",
    "            dataset.at[index, \"language\"] = 'en'\n",
    "            continue  # If already English, move to the next caption\n",
    "        elif detected_language == 'unknown':\n",
    "            to_drop.append(index)  # Drop the row if language is unknown\n",
    "        else:\n",
    "            # Translate to English\n",
    "            translated_text = translate_to_english(row[\"cleaned_caption\"])\n",
    "            if translated_text:\n",
    "                # Re-detect language after translation\n",
    "                new_language = detect_language(translated_text)\n",
    "                if new_language == 'en':\n",
    "                    # Update with the translated text and mark the language as English\n",
    "                    dataset.at[index, \"cleaned_caption\"] = translated_text\n",
    "                    dataset.at[index, \"language\"] = 'en'\n",
    "                else:\n",
    "                    to_drop.append(index)  # Drop if still not English\n",
    "            else:\n",
    "                to_drop.append(index)  # Drop if translation failed\n",
    "\n",
    "    # Drop rows where the language is still unknown or non-English\n",
    "    dataset.drop(to_drop, inplace=True)\n",
    "\n",
    "    return dataset  # Return the cleaned dataset with only English caption\n",
    "\n",
    "# Example usage with your dataset\n",
    "translated_caption = analyze_and_translate_languages_with_google(df)\n",
    "print(translated_caption[['cleaned_caption', 'language']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do sentiment analysis and give sentiment scores using vader nlp\n",
    "import nltk\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Apply sentiment analysis\n",
    "translated_caption[\"sentiment_score\"] = translated_caption[\"cleaned_caption\"].apply(lambda text: analyzer.polarity_scores(text)[\"compound\"])\n",
    "\n",
    "# Display the results\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_caption.to_json(\"sentiment_analysis(reddit).json\", orient=\"records\", date_format=\"iso\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
